{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7y6jfhIVm72H"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%%bash\n",
        "pip install spacy[transformers]\n",
        "pip install spacy_cleaner\n",
        "python3 -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model creation"
      ],
      "metadata": {
        "id": "0EUgA6jFSd9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUusrn6rC2UO",
        "outputId": "5dcfa280-995a-4a7e-c29e-c85bc678eb4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom pipeline components"
      ],
      "metadata": {
        "id": "J0BFipoAbXCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import Language\n",
        "\n",
        "def normalization(doc):\n",
        "  word_list = []\n",
        "  for token in doc:\n",
        "    if not (token.is_punct or token.is_space or (not token.has_vector)):\n",
        "      word_list.append(token.lemma_.lower())\n",
        "\n",
        "  spaces = np.ones(len(word_list))\n",
        "  if len(word_list) > 1:\n",
        "    spaces[-1] = 0 \n",
        "\n",
        "  return spacy.tokens.Doc(doc.vocab, word_list, spaces)\n",
        "\n",
        "def clean_pipe(docs, *args, **kwargs):\n",
        "    for doc in docs:\n",
        "      yield doc\n",
        "\n",
        "normalization.pipe = clean_pipe\n",
        "\n",
        "@Language.factory('normalizer-factory')\n",
        "def normalizer_factory(nlp, name):\n",
        "  return normalization"
      ],
      "metadata": {
        "id": "-8XLZXOPbYyG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the data"
      ],
      "metadata": {
        "id": "1v0c9WrvUCRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pzw1vTcUDD1D",
        "outputId": "a7330507-7410-41db-ed61-b0874f61bea8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"/content/drive/MyDrive/Datasets/text-moderation/merged_train.csv\")\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/Datasets/text-moderation/merged_test.csv\")"
      ],
      "metadata": {
        "id": "LcVdepepiChF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LABELS = [\"quality\", \"toxic\" ,\"spam\"]\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "remove_pipes = [\"tok2vec\", \"parser\", \"ner\"] #\"tagger\" \"attribute_ruler\",\n",
        "for pipe in remove_pipes:\n",
        "  nlp.remove_pipe(pipe)\n",
        "\n",
        "nlp.add_pipe(\"normalizer-factory\", after=\"lemmatizer\")\n",
        "\n",
        "doc_bin_train = spacy.tokens.DocBin()\n",
        "doc_bin_test = spacy.tokens.DocBin()\n",
        "\n",
        "\n",
        "for text, lab in tqdm(nlp.pipe(zip(list(train_data['text']), list(train_data['label'])), as_tuples=True)):\n",
        "    for l in LABELS: text.cats[l] = 0 \n",
        "    text.cats[LABELS[lab]] = True\n",
        "    doc_bin_train.add(text)\n",
        "\n",
        "for text, lab in tqdm(nlp.pipe(zip(list(test_data['text'][:10_000]), list(test_data['label'][:10_000])), as_tuples=True)):\n",
        "    for l in LABELS: text.cats[l] = 0 \n",
        "    text.cats[LABELS[lab]] = True\n",
        "    doc_bin_test.add(text)\n",
        "\n",
        "\n",
        "doc_bin_train.to_disk('./train.spacy')\n",
        "doc_bin_test.to_disk('./test.spacy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucYi0xe1S1mR",
        "outputId": "4999adc0-bc44-4e1d-b94c-aec14ae18710"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18000it [00:52, 345.70it/s] \n",
            "10000it [00:30, 331.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating config file\n"
      ],
      "metadata": {
        "id": "DD6tugjSUEtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = (\"\"\"[paths]\n",
        "train = null\n",
        "dev = null\n",
        "vectors = \"en_core_web_md\"\n",
        "[system]\n",
        "gpu_allocator = \"pytorch\"\n",
        "\n",
        "[nlp]\n",
        "lang = \"en\"\n",
        "pipeline = [\"tok2vec\",\"tagger\", \"attribute_ruler\",\"lemmatizer\", \"normalization\", \"textcat_multilabel\"]\n",
        "batch_size = 1000\n",
        "\n",
        "[components]\n",
        "\n",
        "[components.lemmatizer]\n",
        "source = \"en_core_web_md\"\n",
        "\n",
        "[components.tagger]\n",
        "source = \"en_core_web_md\"\n",
        "\n",
        "[components.attribute_ruler]\n",
        "source = \"en_core_web_md\"\n",
        "\n",
        "\n",
        "[components.normalization]\n",
        "factory = \"normalizer-factory\"\n",
        "\n",
        "[components.tok2vec]\n",
        "source = \"en_core_web_md\"\n",
        "\n",
        "[components.tok2vec.model]\n",
        "@architectures = \"spacy.Tok2Vec.v2\"\n",
        "\n",
        "[components.tok2vec.model.embed]\n",
        "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "attrs = [\"NORM\", \"PREFIX\", \"SUFFIX\", \"SHAPE\"]\n",
        "rows = [5000, 1000, 2500, 2500]\n",
        "include_static_vectors = false\n",
        "\n",
        "[components.tok2vec.model.encode]\n",
        "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
        "width = 96\n",
        "depth = 4\n",
        "window_size = 1\n",
        "maxout_pieces = 3\n",
        "\n",
        "\n",
        "[components.textcat_multilabel]\n",
        "factory = \"textcat_multilabel\"\n",
        "\n",
        "[components.textcat_multilabel.model]\n",
        "@architectures = \"spacy.TextCatCNN.v2\"\n",
        "exclusive_classes = false\n",
        "\n",
        "[components.textcat_multilabel.model.tok2vec]\n",
        "@architectures = \"spacy.HashEmbedCNN.v2\"\n",
        "pretrained_vectors = True\n",
        "width = 96\n",
        "depth = 4\n",
        "embed_size = 2000\n",
        "window_size = 1\n",
        "maxout_pieces = 3\n",
        "subword_features = true\n",
        "\n",
        "\n",
        "[corpora]\n",
        "\n",
        "[corpora.train]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.train}\n",
        "max_length = 0\n",
        "\n",
        "[corpora.dev]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.dev}\n",
        "max_length = 0\n",
        "\n",
        "[training]\n",
        "accumulate_gradient = 3\n",
        "dev_corpus = \"corpora.dev\"\n",
        "train_corpus = \"corpora.train\"\n",
        "frozen_components = [\"tok2vec\", \"normalization\", \"lemmatizer\", \"tagger\", \"attribute_ruler\"]\n",
        "max_epochs = 5\n",
        "\n",
        "[training.optimizer]\n",
        "@optimizers = \"Adam.v1\"\n",
        "\n",
        "[training.optimizer.learn_rate]\n",
        "@schedules = \"warmup_linear.v1\"\n",
        "warmup_steps = 250\n",
        "total_steps = 20000\n",
        "initial_rate = 5e-5\n",
        "\n",
        "[training.batcher]\n",
        "@batchers = \"spacy.batch_by_padded.v1\"\n",
        "discard_oversize = true\n",
        "size = 2000\n",
        "buffer = 512\n",
        "\n",
        "[initialize]\n",
        "vectors = ${paths.vectors} \"\"\")\n",
        "\n",
        "with open(\"./base_config.cfg\", \"w\") as f:\n",
        "  f.write(config)"
      ],
      "metadata": {
        "id": "OC18swlUWHsu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "GeS853xtUGZA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEbNGZY2B4Db",
        "outputId": "a294d987-abca-4075-f9dc-053280dddce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-02-12 08:45:23.350407: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-12 08:45:23.350547: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-12 08:45:23.350558: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-02-12 08:45:25.003226: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "python -m spacy init fill-config ./base_config.cfg ./config.cfg --code normalizer.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./test.spacy --output ./output --code normalizer.py #--gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0jjXmsZEPta",
        "outputId": "310daa75-c156-49db-a1cc-94882050fdf2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer',\n",
            "'normalization', 'textcat_multilabel']\u001b[0m\n",
            "\u001b[38;5;4mℹ Frozen components: ['tok2vec', 'normalization', 'lemmatizer',\n",
            "'tagger', 'attribute_ruler']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TEXTC...  TAG_ACC  LEMMA_ACC  CATS_SCORE  SCORE \n",
            "---  ------  -------------  -------  ---------  ----------  ------\n",
            "  0       0           0.24    11.51      76.80        0.00    0.29\n",
            "  0     200         141.44    11.51      76.80        0.00    0.29\n",
            "  0     400          84.32    11.51      76.80        0.00    0.29\n",
            "  0     600          77.41    11.51      76.80        0.00    0.29\n",
            "  1     800          66.30    11.51      76.80        0.00    0.29\n",
            "  1    1000          63.61    11.51      76.80        0.00    0.29\n",
            "  1    1200          53.12    11.51      76.80        0.00    0.29\n",
            "  2    1400          53.99    11.51      76.80        0.00    0.29\n",
            "  2    1600          52.87    11.51      76.80        0.00    0.29\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-02-12 08:45:36.482923: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-12 08:45:36.483036: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-12 08:45:36.483049: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-02-12 08:45:38.021261: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "[2023-02-12 08:45:41,708] [INFO] Set up nlp object from config\n",
            "INFO:spacy:Set up nlp object from config\n",
            "[2023-02-12 08:45:41,723] [INFO] Pipeline: ['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'normalization', 'textcat_multilabel']\n",
            "INFO:spacy:Pipeline: ['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'normalization', 'textcat_multilabel']\n",
            "[2023-02-12 08:45:41,728] [INFO] Created vocabulary\n",
            "INFO:spacy:Created vocabulary\n",
            "[2023-02-12 08:45:43,591] [INFO] Added vectors: en_core_web_md\n",
            "INFO:spacy:Added vectors: en_core_web_md\n",
            "[2023-02-12 08:45:43,701] [INFO] Finished initializing nlp object\n",
            "INFO:spacy:Finished initializing nlp object\n",
            "[2023-02-12 08:46:19,508] [INFO] Initialized pipeline components: ['textcat_multilabel']\n",
            "INFO:spacy:Initialized pipeline components: ['textcat_multilabel']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing and saving"
      ],
      "metadata": {
        "id": "QCxkILr-UIvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copytree('/content/output/model-best', \"/content/drive/MyDrive/Datasets/text-moderation/best-model-v2\", dirs_exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0CwJT4BjBkGw",
        "outputId": "27897c5c-e7e1-42f0-8a13-714df89c8f7c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Datasets/text-moderation/best-model-v2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1YKCQ9SiLBiN"
      },
      "outputs": [],
      "source": [
        "nlp_test = spacy.load('/content/output/model-best')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_test = pd.concat((\n",
        "    test_data[test_data.label == 0][:1_300], \n",
        "    test_data[test_data.label == 1][:1_300],\n",
        "    test_data[test_data.label == 2][:1_300],\n",
        "))"
      ],
      "metadata": {
        "id": "p9dAauna8PMN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rSkhzW20TVNS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0984a26-717a-4309-865c-11269ceb410c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3900it [00:23, 167.54it/s]\n"
          ]
        }
      ],
      "source": [
        "pred = []\n",
        "for text in tqdm(nlp_test.pipe(list(balanced_test['text']))):\n",
        "  pred.append(np.argmax([text.cats[\"quality\"], text.cats[\"toxic\"], text.cats[\"spam\"]]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, confusion_matrix"
      ],
      "metadata": {
        "id": "VU3TGBbLoyWK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(balanced_test.label, pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WXBIAloqJzT",
        "outputId": "f855bd42-0d0c-40e0-a869-e16b0bc2f41d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8366666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBfuFjXtTeXS",
        "outputId": "13d10ace-8638-45e7-f061-de74d8042747"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8385272783326296"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "f1_score(balanced_test.label, pred,  average = \"weighted\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cf_matrix = confusion_matrix(balanced_test.label, pred)\n",
        "print(cf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9Q8Ju2eqcmL",
        "outputId": "2f50d7ab-f8d3-4b9c-f3d5-43a41565ae41"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1058  212   30]\n",
            " [ 170 1121    9]\n",
            " [  93  123 1084]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the model"
      ],
      "metadata": {
        "id": "jMdpKbPHUMfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copytree('/content/output/model-best', \"/content/drive/MyDrive/Datasets/text-moderation/best-model-CNN-v1\", dirs_exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vHKx7crkvHwK",
        "outputId": "3081d5ef-4e5d-41bb-8355-926340a7cfd0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Datasets/text-moderation/best-model-CNN-v1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_test('this ain\\'t a prank, it\\'s torture, she\\'s not confused she\\'s beyond annoyed...').cats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8cUJHkeD6SQ",
        "outputId": "4199c6ec-fc60-4fc8-c11a-e0591fc0c51f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'quality': 0.7039249539375305,\n",
              " 'toxic': 0.31629300117492676,\n",
              " 'spam': 0.014851176179945469}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UPgF623FEBy-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}